{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Optimisation II:  A simple neural network \n",
    "\n",
    "### Nom(s): Sajid, Habibi\n",
    "### Pr√©nom(s): Badr, Issam\n",
    "### Groupe: B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  60000 images in the train set\n",
      "there are  10000 images in the test set\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load train data\n",
    "#\n",
    "Xtrain = np.load('train-images.npy')\n",
    "Xtrain = np.array([x.ravel()/255 for x in Xtrain])\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0],Xtrain.shape[1],1)\n",
    "Ytrain = np.load('train-labels.npy')\n",
    "targets_train = []\n",
    "\n",
    "#\n",
    "# Convert digits to 10x1 vectors\n",
    "#\n",
    "for lab in Ytrain:\n",
    "    v      = np.zeros((10,1))\n",
    "    v[lab] = 1\n",
    "    targets_train+=[np.array(v)]\n",
    "\n",
    "#\n",
    "# Load test data\n",
    "#\n",
    "Xtest        = np.load('t10k-images.npy')\n",
    "Xtest        = np.array([x.ravel()/255 for x in Xtest])\n",
    "Xtest        = Xtest.reshape(Xtest.shape[0],Xtest.shape[1],1)\n",
    "Ytest        = np.load('t10k-labels.npy')\n",
    "targets_test = []\n",
    "\n",
    "#\n",
    "# Convert digits to 10x1 vectors\n",
    "#\n",
    "for lab in Ytest:\n",
    "    v = np.zeros((10,1))\n",
    "    v[lab]=1\n",
    "    targets_test+=[np.array(v)]\n",
    "#\n",
    "# Outputs\n",
    "#\n",
    "print('there are ',Xtrain.shape[0],'images in the train set')\n",
    "print('there are ',Xtest.shape[0],'images in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the activation function\n",
    "\n",
    " The activation function defines the output of a node given a set of inputs. We use the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\">softmax</a> function defined by\n",
    " \n",
    " $$\\sigma_{\\alpha} : \\mathbb{R}^p\\rightarrow [0,1]^p, \\quad \\mbox{ s.t.} \\quad[\\sigma_{\\alpha}(x)]_i=\\frac{e^{x_i+\\alpha_i}}{\\displaystyle{\\sum_{j=1}^{p}e^{x_j+\\alpha_j}}}\\quad \\forall i=1:p. $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Nonlinear activation function\n",
    "#\n",
    "def softmax(x,alpha):\n",
    "    \"\"\"\n",
    "    Softmax unit activation function \n",
    "    x    : Numpy array\n",
    "    alpha: scalar\n",
    "    \"\"\" \n",
    "    #\n",
    "    # TO DO\n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Example of a plot of the activation function\n",
    "#\n",
    "t     = np.arange(-5,5,0.1)\n",
    "alpha = 0. #np.arange(-50,50,1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(t,softmax(t,alpha))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of a simple neural network\n",
    "\n",
    "We use a one-layer fully-connected neural network with the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\">softmax</a> activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(x,W):\n",
    "    \"\"\"\n",
    "    # One-layer fully connected neural network\n",
    "    # x: image, i.e. 784x1 vector (28x28)\n",
    "    # W: weight matrices of shape 10x784   \n",
    "    \"\"\"\n",
    "    #\n",
    "    # TO DO: return pred (preticted probabilities) \n",
    "    #\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the loss function\n",
    "\n",
    "The loss function is the <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\">cross-entropy</a> defined by \n",
    "\n",
    "$$J(W)=-\\sum_{i=1}^N p_i \\log(q_i(W)),$$ where $N$ is the number of classes, $(p_i)_{i=1:N}$ are the probabilities of  a data from the training set to belong to a class (0 or 1 because the labels are known), and $(q_i(W))_{i=1:N}$ are the predicted probabilities from the model\n",
    "\n",
    "$$\\forall i=1:N, \\quad q_i(W)=[\\sigma_{\\alpha}(Wx)]_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Loss function = Cross-entropy\n",
    "#\n",
    "def cross_entropy(pred,target,x):\n",
    "    \"\"\"\n",
    "    pred:   predicted probabilities (q(W))\n",
    "    target: probabilities (p)\n",
    "    x:      image \n",
    "    \"\"\" \n",
    "    #\n",
    "    # TO DO: return ce,grad (cross_entropy, gradient of the cross-entropy)\n",
    "    #\n",
    "        \n",
    "    return ce,grad\n",
    "#\n",
    "# Main function \n",
    "#\n",
    "def f(W,x,target):\n",
    "    \"\"\"\n",
    "    W:      weights\n",
    "    target: probabilities (p)\n",
    "    x:      image\n",
    "    \"\"\"\n",
    "    #\n",
    "    # TO DO: return ce, grad, pred (cross_entropy, gradient, predicted probabilities)\n",
    "    #\n",
    "    \n",
    "    return ce,grad,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Test information on the gradient with calls of f\n",
    "#\n",
    "\n",
    "# Define weight matrices\n",
    "W      = np.random.rand(10,Xtrain.shape[1])\n",
    "eps    = 1e-8\n",
    "d      = np.random.rand(10,Xtrain.shape[1])\n",
    "Wtilde = w+eps*d\n",
    "\n",
    "# Retrieve the information on the gradients\n",
    "res    = (f(Wtilde,Xtrain[0],targets_train[0])[0]-f(W,Xtrain[0],targets_train[0])[0])/eps\n",
    "print(res)\n",
    "\n",
    "g      = f(W,Xtrain[0],targets_train[0])[1]\n",
    "print(g.T.dot(d.reshape(7840,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to create batches of samples to be used later in the training phase\n",
    "#\n",
    "def create_batches(x,bs):\n",
    "    \"\"\"\n",
    "    x : set to be considered (array)\n",
    "    bs: batch size (scalar)\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    ind     = np.arange(x.shape[0])\n",
    "    random.shuffle(ind)\n",
    "    nbatch  = ind.shape[0]//bs\n",
    "    rest    = ind.shape[0]%bs\n",
    "    \n",
    "    for n in range(nbatch):\n",
    "        batches +=[ind[bs*n:bs*(n+1)]]\n",
    "    \n",
    "    # Put the remaining elements in a last batch\n",
    "    if rest !=0:        \n",
    "        batches += [ind[-rest:]]\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history    = {}\n",
    "eta        = 1e-5 # learning rate\n",
    "momentum   = 0.   # momemtum factor\n",
    "N_EPOCHS   = 10  \n",
    "BatchSizes = [10000,1024,256] # try different batch sizes for the analysis\n",
    "\n",
    "for bs in BatchSizes:\n",
    "    #\n",
    "    # Sensitivity to the batch size to be investigated in the analysis\n",
    "    #\n",
    "    print('batch size=',bs)\n",
    "    \n",
    "    history[bs]={'train loss':[],'train acc':[],'test loss':[0], 'test acc':[0]}\n",
    "    \n",
    "    # Initialization of the weights\n",
    "    w = np.random.rand(10,Xtrain.shape[1])\n",
    "    \n",
    "    for n in range(N_EPOCHS):\n",
    "        # Minimization of the loss function\n",
    "        \n",
    "        Batches=create_batches(Xtrain,bs)\n",
    "        \n",
    "        for batch in Batches:\n",
    "            # Loop on the batches\n",
    "            #\n",
    "            # TO DO\n",
    "            #\n",
    "                   \n",
    "        # Test accuracy at the end of each epoch  \n",
    "        #\n",
    "        # TO DO\n",
    "        #\n",
    "        \n",
    "        print('Epoch number :', n+1,'test accuracy:',history[bs]['test acc'][n+1],'test loss',history[bs]['test loss'][n+1])\n",
    "        \n",
    "\n",
    "    print('\\n')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the evolution of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in BatchSizes:\n",
    "       \n",
    "    n_batch = Xtrain.shape[0]//bs     \n",
    "    if Xtrain.shape[0]%bs!=0:\n",
    "        n_batch+=1\n",
    "        \n",
    "    E  = [n_batch*n for n in np.arange(N_EPOCHS+1)]\n",
    "    Ep = [str(n) for n in np.arange(N_EPOCHS+1)]\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(history[bs]['train loss'],label = 'training loss')\n",
    "    plt.plot(E[1:],history[bs]['test loss'][1:],linewidth=2.5,label = 'test loss')\n",
    "    plt.xticks(E,Ep)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss Value')\n",
    "    #plt.ylim([0,np.max(history[bs]['test loss'])+2])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(f'model trained with a Batch size of {bs} samples and learning rate of {eta}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the evolution of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in BatchSizes:\n",
    "    print(bs)   \n",
    "    n_batch = Xtrain.shape[0]//bs     \n",
    "    if Xtrain.shape[0]%bs!=0:\n",
    "        n_batch+=1\n",
    "        \n",
    "    print(n_batch)\n",
    "    E=[n_batch*n for n in np.arange(N_EPOCHS+1)]\n",
    "    Ep = [str(n) for n in np.arange(N_EPOCHS+1)]\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(history[bs]['train acc'] ,label  = 'training acuracy')\n",
    "    plt.plot(E[1:],history[bs]['test acc'][1:],linewidth=2.5,label = 'test acuracy')\n",
    "    plt.xticks(E,Ep)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.title(f'model trained with a Batch size of {bs} samples and learning rate of {eta}')\n",
    "    plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results\n",
    "\n",
    "Please provide your comments on the sensitivity of the results to the parameters involved in the learning process (batch size, learning rate, momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
